#!/usr/bin/env python3
"""
à¸—à¸”à¸ªà¸­à¸šà¹‚à¸¡à¹€à¸”à¸¥ v3.0 à¸—à¸µà¹ˆà¹€à¸—à¸£à¸™à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§
"""
import os
import sys
import json
import joblib
import numpy as np
import cv2
from pathlib import Path

def load_model_info():
    """à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸¡à¹€à¸”à¸¥"""
    try:
        with open('trained_model/model_info.json', 'r', encoding='utf-8') as f:
            model_info = json.load(f)
        
        with open('ai_models/labels.json', 'r', encoding='utf-8') as f:
            labels = json.load(f)
            
        return model_info, labels
    except Exception as e:
        print(f"âŒ Error loading model info: {e}")
        return None, None

def test_basic_prediction():
    """à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸à¸·à¹‰à¸™à¸à¸²à¸™"""
    print("\nğŸ§ª à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸à¸·à¹‰à¸™à¸à¸²à¸™...")
    
    try:
        # à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥ components
        classifier = joblib.load('trained_model/classifier.joblib')
        scaler = joblib.load('trained_model/scaler.joblib')
        label_encoder = joblib.load('trained_model/label_encoder.joblib')
        
        # à¸«à¸²à¸£à¸¹à¸›à¸—à¸”à¸ªà¸­à¸š
        test_folder = "organized_dataset/splits/test"
        if not os.path.exists(test_folder):
            print("âŒ à¹„à¸¡à¹ˆà¸à¸šà¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸—à¸”à¸ªà¸­à¸š")
            return
        
        test_results = []
        
        # à¸—à¸”à¸ªà¸­à¸šà¸à¸±à¸šà¸£à¸¹à¸›à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸„à¸¥à¸²à¸ª
        for class_folder in os.listdir(test_folder):
            class_path = os.path.join(test_folder, class_folder)
            if not os.path.isdir(class_path):
                continue
            
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µ front/back à¸«à¸£à¸·à¸­à¸£à¸¹à¸›à¸•à¸£à¸‡à¹†
            front_path = os.path.join(class_path, "front")
            back_path = os.path.join(class_path, "back")
            
            # à¸«à¸²à¸£à¸¹à¸›à¸—à¸”à¸ªà¸­à¸š
            test_images = []
            if os.path.exists(front_path):
                front_images = [f for f in os.listdir(front_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if front_images:
                    test_images.append(os.path.join(front_path, front_images[0]))
            
            if os.path.exists(back_path):
                back_images = [f for f in os.listdir(back_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if back_images:
                    test_images.append(os.path.join(back_path, back_images[0]))
            
            # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ front/back à¹ƒà¸«à¹‰à¸«à¸²à¸£à¸¹à¸›à¸•à¸£à¸‡à¹†
            if not test_images:
                direct_images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if direct_images:
                    test_images.append(os.path.join(class_path, direct_images[0]))
            
            if not test_images:
                print(f"\nâš ï¸ à¹„à¸¡à¹ˆà¸à¸šà¸£à¸¹à¸›à¸—à¸”à¸ªà¸­à¸šà¹ƒà¸™à¸„à¸¥à¸²à¸ª: {class_folder}")
                continue
                
            print(f"\nğŸ–¼ï¸ à¸—à¸”à¸ªà¸­à¸šà¸„à¸¥à¸²à¸ª: {class_folder}")
            print(f"   à¸ˆà¸³à¸™à¸§à¸™à¸£à¸¹à¸›à¸—à¸”à¸ªà¸­à¸š: {len(test_images)}")
            
            # à¸—à¸”à¸ªà¸­à¸šà¸à¸±à¸šà¸£à¸¹à¸›à¹à¸£à¸
            test_image_path = test_images[0]
            image_name = os.path.basename(test_image_path)
            print(f"   à¸£à¸¹à¸›: {image_name}")
            
            try:
                # à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸£à¸¹à¸›
                image = cv2.imread(test_image_path)
                if image is None:
                    print(f"   âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹‚à¸«à¸¥à¸”à¸£à¸¹à¸›à¹„à¸”à¹‰")
                    continue
                    
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                # Extract features à¹à¸šà¸šà¸‡à¹ˆà¸²à¸¢ (à¹ƒà¸Šà¹‰à¸§à¸´à¸˜à¸µà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸šà¸•à¸­à¸™à¹€à¸—à¸£à¸™)
                features = extract_simple_features(image)
                
                # Scale features
                features_scaled = scaler.transform(features.reshape(1, -1))
                
                # à¸—à¸³à¸™à¸²à¸¢
                prediction = classifier.predict(features_scaled)[0]
                probabilities = classifier.predict_proba(features_scaled)[0]
                
                # à¹à¸›à¸¥à¸‡à¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™à¸Šà¸·à¹ˆà¸­à¸„à¸¥à¸²à¸ª
                predicted_class = label_encoder.inverse_transform([prediction])[0]
                confidence = float(probabilities[prediction])
                
                print(f"   ğŸ¯ à¸œà¸¥à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢: {predicted_class}")
                print(f"   ğŸ“Š à¸„à¸§à¸²à¸¡à¸¡à¸±à¹ˆà¸™à¹ƒà¸ˆ: {confidence:.2%}")
                
                # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸§à¸²à¸¡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡ (à¸•à¹‰à¸­à¸‡à¸ˆà¸±à¸”à¸à¸²à¸£à¸à¸±à¸š portrait_back)
                actual_class = class_folder.replace('_back', '').replace('_front', '')
                predicted_class_clean = predicted_class.replace('_back', '').replace('_front', '')
                is_correct = predicted_class_clean == actual_class
                print(f"   âœ… à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡: {'YES' if is_correct else 'NO'}")
                
                # à¹à¸ªà¸”à¸‡à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
                print(f"   ğŸ“‹ à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”:")
                all_classes = label_encoder.classes_
                for i, prob in enumerate(probabilities):
                    class_name = all_classes[i]
                    print(f"      {class_name}: {prob:.2%}")
                
                test_results.append({
                    'actual': actual_class,
                    'predicted': predicted_class_clean,
                    'confidence': confidence,
                    'correct': is_correct
                })
                
            except Exception as e:
                print(f"   âŒ Error: {e}")
                import traceback
                traceback.print_exc()
                
        # à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š
        if test_results:
            print(f"\nğŸ“ˆ à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š:")
            correct_count = sum(1 for r in test_results if r['correct'])
            total_count = len(test_results)
            accuracy = correct_count / total_count if total_count > 0 else 0
            avg_confidence = np.mean([r['confidence'] for r in test_results])
            
            print(f"   à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³: {accuracy:.2%} ({correct_count}/{total_count})")
            print(f"   à¸„à¸§à¸²à¸¡à¸¡à¸±à¹ˆà¸™à¹ƒà¸ˆà¹€à¸‰à¸¥à¸µà¹ˆà¸¢: {avg_confidence:.2%}")
            
            print(f"\nğŸ“‹ à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”:")
            for r in test_results:
                status = "âœ…" if r['correct'] else "âŒ"
                print(f"   {status} {r['actual']} -> {r['predicted']} ({r['confidence']:.2%})")
                
    except Exception as e:
        print(f"âŒ Error in testing: {e}")
        import traceback
        traceback.print_exc()

def extract_simple_features(image):
    """Extract features à¹à¸šà¸šà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸šà¸•à¸­à¸™à¹€à¸—à¸£à¸™ (raw pixels)"""
    # Resize to standard size (224x224 à¹€à¸«à¸¡à¸·à¸­à¸™à¸•à¸­à¸™à¹€à¸—à¸£à¸™)
    image_resized = cv2.resize(image, (224, 224))
    
    # Convert to float and normalize (0-1)
    image_normalized = image_resized.astype(np.float32) / 255.0
    
    # Flatten to 1D array (224 * 224 * 3 = 150,528 features)
    features = image_normalized.flatten()
    
    return features

def check_model_performance():
    """à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¹‚à¸¡à¹€à¸”à¸¥"""
    print("\nğŸ“Š à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¹‚à¸¡à¹€à¸”à¸¥:")
    
    model_info, labels = load_model_info()
    if not model_info:
        return
        
    print(f"   ğŸ“ˆ à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³ (Training): {model_info['training_results']['train_accuracy']:.2%}")
    print(f"   ğŸ“ˆ à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³ (Validation): {model_info['training_results']['val_accuracy']:.2%}")
    print(f"   ğŸ“ˆ à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³ (Test): {model_info['training_results']['test_accuracy']:.2%}")
    
    print(f"\nğŸ·ï¸ à¸„à¸¥à¸²à¸ªà¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š ({len(labels)} à¸„à¸¥à¸²à¸ª):")
    for key, value in labels.items():
        print(f"   â€¢ {key}: {value}")

def main():
    """à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸¥à¸±à¸"""
    print("ğŸš€ à¸—à¸”à¸ªà¸­à¸š Amulet-AI Model v3.0")
    print("=" * 60)
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œà¹‚à¸¡à¹€à¸”à¸¥
    required_files = [
        'trained_model/classifier.joblib',
        'trained_model/scaler.joblib',
        'trained_model/label_encoder.joblib',
        'trained_model/model_info.json',
        'ai_models/labels.json'
    ]
    
    print("ğŸ” à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œà¹‚à¸¡à¹€à¸”à¸¥...")
    missing_files = []
    for file_path in required_files:
        if os.path.exists(file_path):
            size = os.path.getsize(file_path)
            print(f"   âœ… {file_path} ({size:,} bytes)")
        else:
            print(f"   âŒ {file_path}")
            missing_files.append(file_path)
    
    if missing_files:
        print(f"\nâŒ à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸‚à¸²à¸”à¸«à¸²à¸¢: {len(missing_files)} à¹„à¸Ÿà¸¥à¹Œ")
        return
    
    # à¹à¸ªà¸”à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¹‚à¸¡à¹€à¸”à¸¥
    check_model_performance()
    
    # à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢
    test_basic_prediction()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™!")
    print("ğŸ’¡ à¹‚à¸¡à¹€à¸”à¸¥ v3.0 à¸à¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™")
    print("ğŸŒ à¹€à¸£à¸´à¹ˆà¸¡ API: python api/main_api_fast.py")
    print("ğŸ–¥ï¸ à¹€à¸£à¸´à¹ˆà¸¡ Frontend: python -m streamlit run frontend/main_streamlit_app.py")

if __name__ == "__main__":
    main()