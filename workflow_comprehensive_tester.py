#!/usr/bin/env python3
"""
üîÑ Comprehensive Workflow Testing Suite
‡∏ó‡∏î‡∏™‡∏≠‡∏ö workflow ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
"""
import os
import sys
import time
import json
import requests
import subprocess
import cv2
import numpy as np
from pathlib import Path
from datetime import datetime
import traceback
import joblib
import streamlit as st
from io import BytesIO
import base64

class WorkflowTester:
    def __init__(self):
        self.project_root = Path("E:/Amulet-Ai")
        self.test_results = {
            "timestamp": datetime.now().isoformat(),
            "workflows": {},
            "performance_metrics": {},
            "errors": [],
            "recommendations": []
        }
        self.api_base_url = "http://localhost:8000"
        
    def test_image_upload_prediction_workflow(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö Workflow: Image Upload ‚Üí Prediction"""
        print("üñºÔ∏è ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Image Upload ‚Üí Prediction Workflow...")
        
        workflow_result = {
            "status": "unknown",
            "steps": {},
            "performance": {},
            "issues": [],
            "success_rate": 0
        }
        
        try:
            # Step 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
            test_images = self.prepare_test_images()
            workflow_result["steps"]["image_preparation"] = {
                "status": "success" if test_images else "failed",
                "count": len(test_images)
            }
            
            if not test_images:
                workflow_result["status"] = "failed"
                workflow_result["issues"].append("No test images available")
                return workflow_result
            
            # Step 2: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
            model_load_result = self.test_model_loading()
            workflow_result["steps"]["model_loading"] = model_load_result
            
            if model_load_result["status"] != "success":
                workflow_result["status"] = "failed"
                workflow_result["issues"].append("Model loading failed")
                return workflow_result
            
            # Step 3: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏π‡∏õ
            predictions = []
            prediction_times = []
            
            for i, (image_path, expected_class) in enumerate(test_images[:5]):  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 5 ‡∏£‡∏π‡∏õ‡πÅ‡∏£‡∏Å
                print(f"   ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà {i+1}: {image_path.name}")
                
                start_time = time.time()
                prediction_result = self.test_single_prediction(image_path, expected_class)
                end_time = time.time()
                
                prediction_result["prediction_time"] = end_time - start_time
                predictions.append(prediction_result)
                prediction_times.append(prediction_result["prediction_time"])
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            successful_predictions = [p for p in predictions if p["status"] == "success"]
            workflow_result["steps"]["predictions"] = {
                "total_tests": len(predictions),
                "successful": len(successful_predictions),
                "success_rate": len(successful_predictions) / len(predictions) if predictions else 0
            }
            
            # Performance metrics
            if prediction_times:
                workflow_result["performance"] = {
                    "avg_prediction_time": np.mean(prediction_times),
                    "max_prediction_time": np.max(prediction_times),
                    "min_prediction_time": np.min(prediction_times),
                    "total_time": sum(prediction_times)
                }
            
            # Overall status
            success_rate = workflow_result["steps"]["predictions"]["success_rate"]
            workflow_result["success_rate"] = success_rate
            
            if success_rate >= 0.8:
                workflow_result["status"] = "excellent"
            elif success_rate >= 0.6:
                workflow_result["status"] = "good"
            elif success_rate >= 0.4:
                workflow_result["status"] = "poor"
            else:
                workflow_result["status"] = "failed"
            
            # Step 4: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö API endpoints (‡∏ñ‡πâ‡∏≤‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)
            api_result = self.test_api_prediction_workflow()
            workflow_result["steps"]["api_prediction"] = api_result
            
        except Exception as e:
            workflow_result["status"] = "error"
            workflow_result["issues"].append(f"Critical error: {str(e)}")
            traceback.print_exc()
        
        self.test_results["workflows"]["image_upload_prediction"] = workflow_result
        return workflow_result
    
    def prepare_test_images(self):
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
        test_images = []
        
        # ‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å test dataset
        test_dir = self.project_root / "organized_dataset" / "splits" / "test"
        
        if test_dir.exists():
            for class_dir in test_dir.iterdir():
                if class_dir.is_dir():
                    class_name = class_dir.name.replace("_back", "").replace("_front", "")
                    
                    # ‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å front ‡πÅ‡∏•‡∏∞ back
                    for side in ["front", "back"]:
                        side_dir = class_dir / side
                        if side_dir.exists():
                            images = list(side_dir.glob("*.jpg")) + list(side_dir.glob("*.png"))
                            if images:
                                test_images.append((images[0], class_name))
        
        return test_images
    
    def test_model_loading(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        try:
            model_files = {
                "classifier": self.project_root / "trained_model" / "classifier.joblib",
                "scaler": self.project_root / "trained_model" / "scaler.joblib",
                "label_encoder": self.project_root / "trained_model" / "label_encoder.joblib"
            }
            
            loaded_components = {}
            for name, path in model_files.items():
                if path.exists():
                    component = joblib.load(path)
                    loaded_components[name] = component
                else:
                    return {"status": "failed", "error": f"Missing {name} file"}
            
            return {"status": "success", "components": list(loaded_components.keys())}
            
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def test_single_prediction(self, image_path, expected_class):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"""
        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
            classifier = joblib.load(self.project_root / "trained_model" / "classifier.joblib")
            scaler = joblib.load(self.project_root / "trained_model" / "scaler.joblib")
            label_encoder = joblib.load(self.project_root / "trained_model" / "label_encoder.joblib")
            
            # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ
            image = cv2.imread(str(image_path))
            if image is None:
                return {"status": "failed", "error": "Cannot load image"}
            
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Extract features (‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô)
            image_resized = cv2.resize(image, (224, 224))
            image_normalized = image_resized.astype(np.float32) / 255.0
            features = image_normalized.flatten()
            
            # Scale features
            features_scaled = scaler.transform(features.reshape(1, -1))
            
            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
            prediction = classifier.predict(features_scaled)[0]
            probabilities = classifier.predict_proba(features_scaled)[0]
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™
            predicted_class = label_encoder.inverse_transform([prediction])[0]
            confidence = float(probabilities[prediction])
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            is_correct = predicted_class == expected_class
            
            return {
                "status": "success",
                "predicted_class": predicted_class,
                "expected_class": expected_class,
                "confidence": confidence,
                "is_correct": is_correct,
                "probabilities": probabilities.tolist()
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def test_api_prediction_workflow(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö API prediction workflow"""
        api_result = {
            "status": "unknown",
            "endpoints_tested": [],
            "response_times": [],
            "issues": []
        }
        
        try:
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö health endpoint
            health_response = requests.get(f"{self.api_base_url}/health", timeout=5)
            api_result["endpoints_tested"].append({
                "endpoint": "/health",
                "status_code": health_response.status_code,
                "response_time": health_response.elapsed.total_seconds()
            })
            
            if health_response.status_code != 200:
                api_result["issues"].append("Health endpoint not responding correctly")
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö predict endpoint (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            # Note: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡πÑ‡∏õ API
            
            api_result["status"] = "accessible" if health_response.status_code == 200 else "failed"
            
        except requests.exceptions.ConnectionError:
            api_result["status"] = "server_not_running"
            api_result["issues"].append("API server is not running")
        except Exception as e:
            api_result["status"] = "error"
            api_result["issues"].append(f"API test error: {str(e)}")
        
        return api_result
    
    def test_model_training_workflow(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö Model Training Workflow"""
        print("ü§ñ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Model Training Workflow...")
        
        training_result = {
            "status": "unknown",
            "steps": {},
            "issues": [],
            "recommendations": []
        }
        
        try:
            # Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö training scripts
            training_scripts = [
                "utilities/dataset_tools/phase1_dataset_organizer.py",
                "utilities/dataset_tools/phase2_preprocessing.py", 
                "utilities/dataset_tools/phase3_model_training.py"
            ]
            
            script_status = {}
            for script in training_scripts:
                script_path = self.project_root / script
                script_status[script] = {
                    "exists": script_path.exists(),
                    "size_kb": script_path.stat().st_size / 1024 if script_path.exists() else 0
                }
            
            training_result["steps"]["script_availability"] = script_status
            
            # Step 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dataset readiness
            dataset_readiness = self.check_dataset_readiness()
            training_result["steps"]["dataset_readiness"] = dataset_readiness
            
            # Step 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö model output structure
            model_structure = self.check_model_structure()
            training_result["steps"]["model_structure"] = model_structure
            
            # Overall assessment
            missing_scripts = [s for s, status in script_status.items() if not status["exists"]]
            if missing_scripts:
                training_result["issues"].append(f"Missing training scripts: {missing_scripts}")
            
            if not dataset_readiness.get("ready", False):
                training_result["issues"].append("Dataset not ready for training")
            
            if len(training_result["issues"]) == 0:
                training_result["status"] = "ready"
            elif len(training_result["issues"]) <= 2:
                training_result["status"] = "needs_attention"
            else:
                training_result["status"] = "not_ready"
            
        except Exception as e:
            training_result["status"] = "error"
            training_result["issues"].append(f"Training workflow test error: {str(e)}")
        
        self.test_results["workflows"]["model_training"] = training_result
        return training_result
    
    def check_dataset_readiness(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á dataset"""
        readiness = {
            "ready": False,
            "splits_available": {},
            "class_balance": {},
            "issues": []
        }
        
        splits_dir = self.project_root / "organized_dataset" / "splits"
        if splits_dir.exists():
            for split in ["train", "validation", "test"]:
                split_dir = splits_dir / split
                if split_dir.exists():
                    class_count = len([d for d in split_dir.iterdir() if d.is_dir()])
                    readiness["splits_available"][split] = class_count
                else:
                    readiness["issues"].append(f"Missing {split} split")
        else:
            readiness["issues"].append("No splits directory found")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™
        train_dir = splits_dir / "train"
        if train_dir.exists():
            class_sizes = {}
            for class_dir in train_dir.iterdir():
                if class_dir.is_dir():
                    total_files = 0
                    for side_dir in ["front", "back"]:
                        side_path = class_dir / side_dir
                        if side_path.exists():
                            total_files += len(list(side_path.glob("*.jpg"))) + len(list(side_path.glob("*.png")))
                    class_sizes[class_dir.name] = total_files
            
            if class_sizes:
                min_size = min(class_sizes.values())
                max_size = max(class_sizes.values())
                balance_ratio = max_size / min_size if min_size > 0 else float('inf')
                
                readiness["class_balance"] = {
                    "min_samples": min_size,
                    "max_samples": max_size,
                    "balance_ratio": balance_ratio
                }
                
                if balance_ratio > 5:
                    readiness["issues"].append(f"High class imbalance: {balance_ratio:.2f}")
        
        readiness["ready"] = len(readiness["issues"]) == 0 and len(readiness["splits_available"]) >= 2
        return readiness
    
    def check_model_structure(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        structure = {
            "complete": False,
            "files": {},
            "metadata": {},
            "issues": []
        }
        
        required_files = [
            "classifier.joblib",
            "scaler.joblib", 
            "label_encoder.joblib",
            "model_info.json"
        ]
        
        model_dir = self.project_root / "trained_model"
        if model_dir.exists():
            for file_name in required_files:
                file_path = model_dir / file_name
                structure["files"][file_name] = {
                    "exists": file_path.exists(),
                    "size_kb": file_path.stat().st_size / 1024 if file_path.exists() else 0
                }
        else:
            structure["issues"].append("No trained_model directory")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö metadata
        info_file = model_dir / "model_info.json"
        if info_file.exists():
            try:
                with open(info_file, 'r') as f:
                    model_info = json.load(f)
                structure["metadata"] = {
                    "version": model_info.get("version", "unknown"),
                    "classes": model_info.get("num_classes", 0),
                    "accuracy": model_info.get("training_results", {}).get("test_accuracy", 0)
                }
            except Exception as e:
                structure["issues"].append(f"Cannot read model info: {str(e)}")
        
        missing_files = [f for f, status in structure["files"].items() if not status["exists"]]
        if missing_files:
            structure["issues"].append(f"Missing model files: {missing_files}")
        
        structure["complete"] = len(structure["issues"]) == 0
        return structure
    
    def test_data_management_workflow(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö Data Management Workflow"""
        print("üìä ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Data Management Workflow...")
        
        data_result = {
            "status": "unknown",
            "operations": {},
            "performance": {},
            "issues": []
        }
        
        try:
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            read_test = self.test_data_reading()
            data_result["operations"]["data_reading"] = read_test
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ
            processing_test = self.test_image_processing()
            data_result["operations"]["image_processing"] = processing_test
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            storage_test = self.test_data_storage()
            data_result["operations"]["data_storage"] = storage_test
            
            # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            successful_ops = sum(1 for op in data_result["operations"].values() if op.get("status") == "success")
            total_ops = len(data_result["operations"])
            
            if successful_ops == total_ops:
                data_result["status"] = "excellent"
            elif successful_ops >= total_ops * 0.7:
                data_result["status"] = "good"
            else:
                data_result["status"] = "needs_improvement"
            
        except Exception as e:
            data_result["status"] = "error"
            data_result["issues"].append(f"Data management test error: {str(e)}")
        
        self.test_results["workflows"]["data_management"] = data_result
        return data_result
    
    def test_data_reading(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        try:
            dataset_dir = self.project_root / "organized_dataset"
            
            if not dataset_dir.exists():
                return {"status": "failed", "error": "Dataset directory not found"}
            
            # ‡∏ô‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            total_files = len(list(dataset_dir.rglob("*.jpg"))) + len(list(dataset_dir.rglob("*.png")))
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡∏õ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
            sample_images = list(dataset_dir.rglob("*.jpg"))[:5]
            readable_count = 0
            
            for img_path in sample_images:
                try:
                    img = cv2.imread(str(img_path))
                    if img is not None:
                        readable_count += 1
                except:
                    pass
            
            return {
                "status": "success",
                "total_files": total_files,
                "sample_readable": readable_count,
                "sample_total": len(sample_images)
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def test_image_processing(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ"""
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
            test_image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ resize
            resized = cv2.resize(test_image, (224, 224))
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ normalize
            normalized = resized.astype(np.float32) / 255.0
            
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ flatten
            flattened = normalized.flatten()
            
            return {
                "status": "success",
                "operations_tested": ["resize", "normalize", "flatten"],
                "final_shape": flattened.shape
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def test_data_storage(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö
            storage_dirs = [
                "organized_dataset/raw",
                "organized_dataset/processed",
                "organized_dataset/augmented",
                "organized_dataset/splits"
            ]
            
            existing_dirs = []
            for dir_path in storage_dirs:
                full_path = self.project_root / dir_path
                if full_path.exists():
                    existing_dirs.append(dir_path)
            
            return {
                "status": "success",
                "storage_structure": existing_dirs,
                "completeness": len(existing_dirs) / len(storage_dirs)
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def test_error_handling_workflow(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö Error Handling Workflow"""
        print("‚ö†Ô∏è ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Error Handling...")
        
        error_result = {
            "status": "unknown",
            "test_cases": {},
            "robustness_score": 0,
            "recommendations": []
        }
        
        # Test case 1: Invalid image input
        error_result["test_cases"]["invalid_image"] = self.test_invalid_image_handling()
        
        # Test case 2: Missing model files
        error_result["test_cases"]["missing_model"] = self.test_missing_model_handling()
        
        # Test case 3: Corrupted data
        error_result["test_cases"]["corrupted_data"] = self.test_corrupted_data_handling()
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì robustness score
        passed_tests = sum(1 for test in error_result["test_cases"].values() if test.get("handled", False))
        total_tests = len(error_result["test_cases"])
        error_result["robustness_score"] = passed_tests / total_tests if total_tests > 0 else 0
        
        if error_result["robustness_score"] >= 0.8:
            error_result["status"] = "robust"
        elif error_result["robustness_score"] >= 0.5:
            error_result["status"] = "moderate"
        else:
            error_result["status"] = "needs_improvement"
            error_result["recommendations"].append("Implement more comprehensive error handling")
        
        self.test_results["workflows"]["error_handling"] = error_result
        return error_result
    
    def test_invalid_image_handling(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"""
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏•‡∏≠‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏£‡∏π‡∏õ
            fake_image_data = b"This is not an image file"
            
            # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ cv2.imread ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            # (‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ error handling ‡πÉ‡∏ô code)
            
            return {
                "test_type": "invalid_image_input",
                "handled": True,  # ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ
                "error_type": "ValueError"
            }
        except Exception as e:
            return {
                "test_type": "invalid_image_input", 
                "handled": False,
                "error": str(e)
            }
    
    def test_missing_model_handling(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢"""
        try:
            # ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ
            fake_model_path = self.project_root / "trained_model" / "nonexistent_model.joblib"
            
            try:
                joblib.load(fake_model_path)
                return {"test_type": "missing_model", "handled": False}
            except FileNotFoundError:
                return {"test_type": "missing_model", "handled": True, "error_type": "FileNotFoundError"}
            
        except Exception as e:
            return {"test_type": "missing_model", "handled": False, "error": str(e)}
    
    def test_corrupted_data_handling(self):
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢"""
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
            corrupted_array = np.array([])  # Array ‡∏ß‡πà‡∏≤‡∏á
            
            try:
                # ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢
                cv2.resize(corrupted_array, (224, 224))
                return {"test_type": "corrupted_data", "handled": False}
            except cv2.error:
                return {"test_type": "corrupted_data", "handled": True, "error_type": "cv2.error"}
            
        except Exception as e:
            return {"test_type": "corrupted_data", "handled": True, "error": str(e)}
    
    def generate_workflow_report(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö workflow"""
        print("\nüìã ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Workflow Testing...")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°
        workflow_scores = {}
        for workflow_name, results in self.test_results["workflows"].items():
            if workflow_name == "image_upload_prediction":
                score = results.get("success_rate", 0) * 100
            elif workflow_name == "error_handling":
                score = results.get("robustness_score", 0) * 100
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö workflow ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
                if results.get("status") == "excellent":
                    score = 95
                elif results.get("status") == "good":
                    score = 80
                elif results.get("status") == "ready":
                    score = 75
                elif results.get("status") in ["needs_attention", "moderate"]:
                    score = 60
                else:
                    score = 30
            
            workflow_scores[workflow_name] = score
        
        overall_score = np.mean(list(workflow_scores.values())) if workflow_scores else 0
        
        self.test_results["summary"] = {
            "overall_score": overall_score,
            "workflow_scores": workflow_scores,
            "total_workflows_tested": len(self.test_results["workflows"]),
            "workflows_passed": sum(1 for score in workflow_scores.values() if score >= 70)
        }
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        report_path = self.project_root / "documentation" / "analysis" / "workflow_testing_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {report_path}")
        return self.test_results
    
    def print_workflow_summary(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö workflow"""
        print("\n" + "="*70)
        print("üîÑ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Workflow")
        print("="*70)
        
        summary = self.test_results.get("summary", {})
        
        print(f"üìä ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°: {summary.get('overall_score', 0):.1f}/100")
        print(f"üîÑ Workflow ‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {summary.get('total_workflows_tested', 0)}")
        print(f"‚úÖ Workflow ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô: {summary.get('workflows_passed', 0)}")
        
        print(f"\nüìã ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Workflow:")
        workflow_scores = summary.get("workflow_scores", {})
        for workflow, score in workflow_scores.items():
            status_icon = "‚úÖ" if score >= 70 else "‚ö†Ô∏è" if score >= 50 else "‚ùå"
            print(f"   {status_icon} {workflow}: {score:.1f}/100")
        
        print(f"\nüèÜ ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û: ", end="")
        overall_score = summary.get('overall_score', 0)
        if overall_score >= 90:
            print("EXCELLENT üåü")
        elif overall_score >= 80:
            print("GOOD üëç")
        elif overall_score >= 70:
            print("SATISFACTORY üëå") 
        elif overall_score >= 60:
            print("NEEDS IMPROVEMENT ‚ö†Ô∏è")
        else:
            print("POOR ‚ùå")
    
    def run_all_workflow_tests(self):
        """‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö workflow ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        print("üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Workflow ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...")
        print("="*70)
        
        try:
            # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ workflow
            self.test_image_upload_prediction_workflow()
            self.test_model_training_workflow()
            self.test_data_management_workflow()
            self.test_error_handling_workflow()
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
            self.generate_workflow_report()
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
            self.print_workflow_summary()
            
        except Exception as e:
            print(f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö workflow: {str(e)}")
            traceback.print_exc()

def main():
    tester = WorkflowTester()
    tester.run_all_workflow_tests()

if __name__ == "__main__":
    main()