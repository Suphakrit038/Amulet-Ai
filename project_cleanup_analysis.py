#!/usr/bin/env python3
"""
üîç Project Cleanup & Analysis Tool
‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏∞‡∏ö‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ Amulet-AI ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡πÅ‡∏•‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
"""

import os
import json
import shutil
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProjectAnalyzer:
    def __init__(self, root_path: str):
        self.root = Path(root_path)
        self.analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "duplicates": [],
            "secondary_systems": [],
            "unnecessary_files": [],
            "missing_connections": [],
            "project_weaknesses": [],
            "recommendations": []
        }
    
    def analyze_project(self):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        logger.info("üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ Amulet-AI")
        
        # 1. ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        self._find_duplicate_systems()
        
        # 2. ‡∏´‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á/‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        self._find_secondary_systems()
        
        # 3. ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        self._find_unnecessary_files()
        
        # 4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå
        self._analyze_file_connections()
        
        # 5. ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ
        self._identify_project_weaknesses()
        
        # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        self._generate_recommendations()
        
        return self.analysis_results
    
    def _find_duplicate_systems(self):
        """‡∏´‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô"""
        logger.info("üìã ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô...")
        
        # ‡πÑ‡∏ü‡∏•‡πå AI Models ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        ai_models = list(self.root.glob("ai_models/*.py"))
        if len(ai_models) > 2:  # ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ __init__.py ‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß
            for model in ai_models:
                if model.name != "__init__.py":
                    self.analysis_results["duplicates"].append({
                        "type": "AI Model",
                        "file": str(model),
                        "reason": "‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå AI model - ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏Å"
                    })
        
        # ‡πÑ‡∏ü‡∏•‡πå trained models ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        trained_dirs = [d for d in self.root.iterdir() if d.is_dir() and "trained_model" in d.name]
        if len(trained_dirs) > 1:
            for trained_dir in trained_dirs:
                self.analysis_results["duplicates"].append({
                    "type": "Trained Model",
                    "file": str(trained_dir),
                    "reason": "‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ trained model directories - ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞ current version"
                })
        
        # API ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        api_files = list(self.root.glob("backend/api/*.py"))
        api_systems = [f for f in api_files if "api" in f.name]
        if len(api_systems) > 1:
            for api in api_systems:
                self.analysis_results["duplicates"].append({
                    "type": "API System",
                    "file": str(api),
                    "reason": "‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ API implementations"
                })
    
    def _find_secondary_systems(self):
        """‡∏´‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"""
        logger.info("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å...")
        
        secondary_patterns = [
            ("lean", "‡∏£‡∏∞‡∏ö‡∏ö Lean - ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Production system"),
            ("v3", "Version 3 - ‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡πâ‡∏≤‡∏°‡∏µ v4"),
            ("production_ready", "Production Ready - ‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö Enhanced Production"),
            ("secondary", "‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á"),
            ("alternative", "‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"),
            ("backup", "‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"),
            ("temp", "‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß"),
            ("test", "‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô)"),
            ("old", "‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤")
        ]
        
        for pattern, reason in secondary_patterns:
            matches = list(self.root.rglob(f"*{pattern}*"))
            for match in matches:
                if match.is_file() and not match.name.startswith('.'):
                    self.analysis_results["secondary_systems"].append({
                        "file": str(match),
                        "pattern": pattern,
                        "reason": reason
                    })
    
    def _find_unnecessary_files(self):
        """‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
        logger.info("üóëÔ∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô...")
        
        unnecessary_files = [
            "miniconda.exe",
            "*.pyc",
            "__pycache__",
            "*.log",
            ".DS_Store",
            "Thumbs.db",
            "desktop.ini",
            "*.tmp",
            "*.temp"
        ]
        
        doc_files = [
            "PHASE3_FINAL_REPORT.md",
            "ENHANCED_SYSTEM_FINAL_REPORT.md", 
            "PERSONA_TECHNICAL_SOLUTIONS.md",
            "PERSONA_SOLUTIONS_QUICK_REFERENCE.md",
            "persona_solutions_summary.json"
        ]
        
        # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        for pattern in unnecessary_files:
            matches = list(self.root.rglob(pattern))
            for match in matches:
                self.analysis_results["unnecessary_files"].append({
                    "file": str(match),
                    "type": "System file",
                    "reason": "‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞"
                })
        
        # ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô
        for doc in doc_files:
            doc_path = self.root / doc
            if doc_path.exists():
                self.analysis_results["unnecessary_files"].append({
                    "file": str(doc_path),
                    "type": "Documentation",
                    "reason": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ - ‡∏Ñ‡∏ß‡∏£‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô README ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"
                })
    
    def _analyze_file_connections(self):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        logger.info("üîó ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå...")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö import ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢
        python_files = list(self.root.rglob("*.py"))
        
        for py_file in python_files:
            if py_file.name.startswith('.'):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # ‡∏´‡∏≤ import ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢
                if "optimized_model" in content:
                    if not (self.root / "ai_models" / "optimized_model.py").exists():
                        self.analysis_results["missing_connections"].append({
                            "file": str(py_file),
                            "issue": "Import optimized_model ‡πÅ‡∏ï‡πà‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏û‡∏ö",
                            "severity": "High"
                        })
                
                if "lean_model" in content:
                    if not (self.root / "ai_models" / "lean_model.py").exists():
                        self.analysis_results["missing_connections"].append({
                            "file": str(py_file),
                            "issue": "Import lean_model ‡πÅ‡∏ï‡πà‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏û‡∏ö",
                            "severity": "Medium"
                        })
                        
            except Exception as e:
                logger.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {py_file}: {e}")
    
    def _identify_project_weaknesses(self):
        """‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ"""
        logger.info("‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ...")
        
        weaknesses = []
        
        # 1. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        weaknesses.append({
            "category": "Architecture",
            "issue": "‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ô",
            "description": "‡∏°‡∏µ v3, v4, enhanced, production ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏ö‡∏™‡∏ô",
            "severity": "High",
            "impact": "‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏™‡∏±‡∏ö‡∏™‡∏ô, maintenance ‡∏¢‡∏≤‡∏Å"
        })
        
        # 2. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏ô‡πâ‡∏≠‡∏¢
        dataset_path = self.root / "dataset_optimized"
        if dataset_path.exists():
            train_dirs = list((dataset_path / "train").glob("*"))
            if train_dirs:
                sample_count = len(list(train_dirs[0].glob("*.jpg")))
                if sample_count < 50:
                    weaknesses.append({
                        "category": "Data",
                        "issue": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡∏°‡∏µ‡∏ô‡πâ‡∏≠‡∏¢",
                        "description": f"‡∏°‡∏µ‡πÄ‡∏û‡∏µ‡∏¢‡∏á {sample_count} ‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™ - ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production",
                        "severity": "Critical",
                        "impact": "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏≤‡∏à overfit, ‡πÑ‡∏°‡πà generalize ‡πÑ‡∏î‡πâ‡∏î‡∏µ"
                    })
        
        # 3. ‡∏´‡∏•‡∏∏‡∏î‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡πÄ‡∏î‡∏¥‡∏°
        weaknesses.append({
            "category": "Project Focus",
            "issue": "‡∏´‡∏•‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡πÄ‡∏î‡∏¥‡∏°",
            "description": "‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏û‡∏£‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô tech showcase ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô",
            "severity": "High",
            "impact": "‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏¢‡∏≤‡∏Å, ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"
        })
        
        # 4. Over-engineering
        weaknesses.append({
            "category": "Engineering",
            "issue": "Over-engineering",
            "description": "‡∏°‡∏µ features ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏ä‡πà‡∏ô multiple personas, complex monitoring",
            "severity": "Medium", 
            "impact": "‡∏£‡∏∞‡∏ö‡∏ö‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô, ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÅ‡∏•‡∏∞ maintain ‡∏¢‡∏≤‡∏Å"
        })
        
        # 5. ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô
        doc_count = len(list(self.root.glob("*.md")))
        if doc_count > 5:
            weaknesses.append({
                "category": "Documentation",
                "issue": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ",
                "description": f"‡∏°‡∏µ {doc_count} ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ - ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ",
                "severity": "Low",
                "impact": "‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏±‡∏ö‡∏™‡∏ô, ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏≠‡∏∞‡πÑ‡∏£"
            })
        
        # 6. Dataset ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
        dataset_dirs = [d for d in self.root.iterdir() if d.is_dir() and "dataset" in d.name]
        if len(dataset_dirs) > 1:
            weaknesses.append({
                "category": "Data Management",
                "issue": "Dataset ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô",
                "description": f"‡∏°‡∏µ {len(dataset_dirs)} datasets - ‡∏™‡∏¥‡πâ‡∏ô‡πÄ‡∏õ‡∏•‡∏∑‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà",
                "severity": "Medium",
                "impact": "‡πÉ‡∏ä‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏¥‡∏™‡∏Å‡πå‡∏°‡∏≤‡∏Å, ‡∏™‡∏±‡∏ö‡∏™‡∏ô"
            })
        
        self.analysis_results["project_weaknesses"] = weaknesses
    
    def _generate_recommendations(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥"""
        logger.info("üí° ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥...")
        
        recommendations = [
            {
                "priority": "Critical",
                "action": "‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß",
                "description": "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å enhanced_production_system.py ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡∏•‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠",
                "benefit": "‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏ö‡∏™‡∏ô, ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ maintain"
            },
            {
                "priority": "High", 
                "action": "‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å",
                "description": "‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 100-500 ‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production",
                "benefit": "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏∂‡πâ‡∏ô"
            },
            {
                "priority": "High",
                "action": "‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á UI",
                "description": "‡∏ó‡∏≥ UI ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà tech experts",
                "benefit": "‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢, ‡∏ï‡∏£‡∏á‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå"
            },
            {
                "priority": "Medium",
                "action": "‡∏£‡∏ß‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô README ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß",
                "description": "‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà README.md ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
                "benefit": "‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡πà‡∏≤‡∏¢"
            },
            {
                "priority": "Medium",
                "action": "‡∏•‡∏ö trained models ‡πÄ‡∏Å‡πà‡∏≤",
                "description": "‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà model ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á",
                "benefit": "‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà, ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏ö‡∏™‡∏ô"
            },
            {
                "priority": "Low",
                "action": "‡∏à‡∏±‡∏î‡∏£‡∏∞‡∏ö‡∏ö dependencies",
                "description": "‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó requirements.txt ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á",
                "benefit": "‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏á‡πà‡∏≤‡∏¢, ‡πÑ‡∏°‡πà‡∏°‡∏µ dependency conflicts"
            }
        ]
        
        self.analysis_results["recommendations"] = recommendations
    
    def generate_cleanup_script(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏£‡∏∞‡∏ö‡∏ö"""
        
        cleanup_actions = []
        
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥
        for dup in self.analysis_results["duplicates"]:
            cleanup_actions.append(f"# ‡∏•‡∏ö duplicate: {dup['file']}")
            cleanup_actions.append(f"rm -rf '{dup['file']}'")
        
        # ‡∏•‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á
        for sec in self.analysis_results["secondary_systems"]:
            cleanup_actions.append(f"# ‡∏•‡∏ö secondary system: {sec['file']}")
            cleanup_actions.append(f"rm -rf '{sec['file']}'")
        
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        for unnecessary in self.analysis_results["unnecessary_files"]:
            cleanup_actions.append(f"# ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {unnecessary['file']}")
            cleanup_actions.append(f"rm -rf '{unnecessary['file']}'")
        
        return "\n".join(cleanup_actions)
    
    def save_analysis(self, output_path: str = "project_analysis_report.json"):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        self.analysis_results["summary"] = {
            "total_duplicates": len(self.analysis_results["duplicates"]),
            "total_secondary_systems": len(self.analysis_results["secondary_systems"]),
            "total_unnecessary_files": len(self.analysis_results["unnecessary_files"]),
            "total_weaknesses": len(self.analysis_results["project_weaknesses"]),
            "critical_issues": len([w for w in self.analysis_results["project_weaknesses"] if w["severity"] == "Critical"]),
            "cleanup_script": self.generate_cleanup_script()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏µ‡πà: {output_path}")

def main():
    """‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
    
    current_dir = Path(__file__).parent
    analyzer = ProjectAnalyzer(current_dir)
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ
    results = analyzer.analyze_project()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
    analyzer.save_analysis("project_analysis_report.json")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
    print("\n" + "="*60)
    print("üîç PROJECT ANALYSIS SUMMARY")
    print("="*60)
    
    print(f"üìä ‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:")
    print(f"   ‚Ä¢ ‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥: {len(results['duplicates'])} ‡πÑ‡∏ü‡∏•‡πå")
    print(f"   ‚Ä¢ ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á: {len(results['secondary_systems'])} ‡πÑ‡∏ü‡∏•‡πå")
    print(f"   ‚Ä¢ ‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {len(results['unnecessary_files'])} ‡πÑ‡∏ü‡∏•‡πå")
    print(f"   ‚Ä¢ ‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ: {len(results['project_weaknesses'])} ‡∏à‡∏∏‡∏î")
    
    critical_issues = [w for w in results['project_weaknesses'] if w['severity'] == 'Critical']
    if critical_issues:
        print(f"\n‚ö†Ô∏è ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ß‡∏¥‡∏Å‡∏§‡∏ï‡∏¥ ({len(critical_issues)} ‡∏à‡∏∏‡∏î):")
        for issue in critical_issues:
            print(f"   ‚Ä¢ {issue['issue']}: {issue['description']}")
    
    print(f"\nüí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:")
    for rec in results['recommendations'][:3]:
        print(f"   ‚Ä¢ [{rec['priority']}] {rec['action']}")
    
    print(f"\nüìÑ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ï‡πá‡∏°: project_analysis_report.json")
    print("="*60)

if __name__ == "__main__":
    main()