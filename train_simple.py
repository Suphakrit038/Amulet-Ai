"""
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á (Synthetic Data)
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î Mock mode ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ AI ‡∏à‡∏£‡∏¥‡∏á
"""
import os
import json
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import MobileNetV3Small
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# GPU configuration
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logger.info(f"üéÆ Found {len(gpus)} GPU(s)")
    except RuntimeError as e:
        logger.error(f"‚ùå GPU configuration error: {e}")
else:
    logger.info("üñ•Ô∏è Using CPU for training")

class SyntheticDataGenerator:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å"""
    
    def __init__(self, num_samples=1000, image_size=(224, 224)):
        self.num_samples = num_samples
        self.image_size = image_size
        self.num_classes = 4
        self.class_names = {
            0: "‡∏´‡∏•‡∏ß‡∏á‡∏û‡πà‡∏≠‡∏Å‡∏ß‡∏¢‡πÅ‡∏´‡∏ß‡∏Å‡∏°‡πà‡∏≤‡∏ô",
            1: "‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏ê‡∏≤‡∏ô‡∏ö‡∏±‡∏ß", 
            2: "‡∏ê‡∏≤‡∏ô‡∏™‡∏¥‡∏á‡∏´‡πå",
            3: "‡∏™‡∏µ‡∏ß‡∏•‡∏µ"
        }
    
    def generate_class_pattern(self, class_id):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™"""
        img = np.random.rand(*self.image_size, 3)
        
        if class_id == 0:  # ‡∏´‡∏•‡∏ß‡∏á‡∏û‡πà‡∏≠‡∏Å‡∏ß‡∏¢‡πÅ‡∏´‡∏ß‡∏Å‡∏°‡πà‡∏≤‡∏ô
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°‡∏Å‡∏•‡∏≤‡∏á (‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏û‡∏£‡∏∞)
            center_y, center_x = self.image_size[0]//2, self.image_size[1]//2
            img[center_y-50:center_y+50, center_x-30:center_x+30, :] *= 0.3
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏µ‡∏ó‡∏≠‡∏á
            img[:, :, 1] *= 1.2  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏™‡∏µ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á
            
        elif class_id == 1:  # ‡πÇ‡∏û‡∏ò‡∏¥‡πå‡∏ê‡∏≤‡∏ô‡∏ö‡∏±‡∏ß
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏á‡∏Å‡∏•‡∏° (‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏ê‡∏≤‡∏ô‡∏ö‡∏±‡∏ß)
            center_y, center_x = self.image_size[0]//2, self.image_size[1]//2
            y, x = np.ogrid[:self.image_size[0], :self.image_size[1]]
            mask = (x - center_x)**2 + (y - center_y)**2 <= 60**2
            img[mask] *= 0.4
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏µ‡πÅ‡∏î‡∏á
            img[:, :, 0] *= 1.3
            
        elif class_id == 2:  # ‡∏ê‡∏≤‡∏ô‡∏™‡∏¥‡∏á‡∏´‡πå
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏™‡∏≤‡∏°‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏° (‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏ê‡∏≤‡∏ô‡∏™‡∏¥‡∏á‡∏´‡πå)
            center_y, center_x = self.image_size[0]//2, self.image_size[1]//2
            for i in range(40):
                img[center_y+i, center_x-i:center_x+i, :] *= 0.2
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô
            img[:, :, 2] *= 1.4
            
        elif class_id == 3:  # ‡∏™‡∏µ‡∏ß‡∏•‡∏µ
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏≤‡∏¢‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á (‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏à‡∏µ‡∏ß‡∏£)
            for i in range(0, self.image_size[1], 20):
                img[:, i:i+5, :] *= 0.5
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
            img[:, :, 1] *= 1.1
            img[:, :, 2] *= 0.8
        
        return img
    
    def generate_dataset(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡∏à‡∏≥‡∏•‡∏≠‡∏á"""
        X = []
        y = []
        
        samples_per_class = self.num_samples // self.num_classes
        
        for class_id in range(self.num_classes):
            for _ in range(samples_per_class):
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
                img = self.generate_class_pattern(class_id)
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏£‡∏ö‡∏Å‡∏ß‡∏ô
                noise = np.random.normal(0, 0.1, img.shape)
                img = np.clip(img + noise, 0, 1)
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° data augmentation ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
                if np.random.rand() > 0.5:
                    img = np.fliplr(img)  # flip horizontal
                
                X.append(img)
                y.append(class_id)
        
        X = np.array(X, dtype=np.float32)
        y = tf.keras.utils.to_categorical(y, self.num_classes)
        
        logger.info(f"‚úÖ Generated {len(X)} synthetic samples")
        return X, y

def create_lightweight_model():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏ï‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á"""
    logger.info("üèóÔ∏è Creating lightweight AI model")
    
    # ‡πÉ‡∏ä‡πâ MobileNetV3Small ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    base_model = MobileNetV3Small(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
    
    # Freeze base model
    base_model.trainable = False
    
    # Build classification head
    inputs = tf.keras.Input(shape=(224, 224, 3))
    x = base_model(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.2)(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(0.1)(x)
    outputs = layers.Dense(4, activation='softmax')(x)
    
    model = Model(inputs, outputs)
    
    # Compile
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy', 'top_2_accuracy']
    )
    
    logger.info(f"‚úÖ Model created with {model.count_params():,} parameters")
    return model

def train_model():
    """‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á"""
    logger.info("üöÄ Starting model training with synthetic data")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á
    data_gen = SyntheticDataGenerator(num_samples=800)
    X, y = data_gen.generate_dataset()
    
    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/validation
    split_idx = int(0.8 * len(X))
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    logger.info(f"üìä Training samples: {len(X_train)}")
    logger.info(f"üìä Validation samples: {len(X_val)}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
    model = create_lightweight_model()
    
    # Callbacks
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=5,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7
        )
    ]
    
    # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    logger.info("üìö Phase 1: Training with frozen base model")
    history1 = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=15,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    
    # Fine-tuning
    logger.info("üîß Phase 2: Fine-tuning")
    base_model = model.layers[1]  # Get the base model
    base_model.trainable = True
    
    # Freeze early layers, only train last few
    for layer in base_model.layers[:-10]:
        layer.trainable = False
    
    # Recompile with lower learning rate
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss='categorical_crossentropy',
        metrics=['accuracy', 'top_2_accuracy']
    )
    
    history2 = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=10,
        batch_size=32,
        callbacks=callbacks,
        initial_epoch=len(history1.history['loss']),
        verbose=1
    )
    
    # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
    val_loss, val_accuracy, val_top2 = model.evaluate(X_val, y_val, verbose=0)
    logger.info(f"üìä Final Results:")
    logger.info(f"   - Accuracy: {val_accuracy:.3f}")
    logger.info(f"   - Top-2 Accuracy: {val_top2:.3f}")
    logger.info(f"   - Loss: {val_loss:.3f}")
    
    return model, data_gen.class_names

def save_model(model, class_names):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ labels"""
    logger.info("üíæ Saving model and labels")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
    os.makedirs("models", exist_ok=True)
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    model_path = "models/amulet_model.h5"
    model.save(model_path)
    logger.info(f"‚úÖ Model saved to {model_path}")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å labels
    labels_path = "labels.json"
    with open(labels_path, 'w', encoding='utf-8') as f:
        json.dump(class_names, f, ensure_ascii=False, indent=2)
    logger.info(f"‚úÖ Labels saved to {labels_path}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á TFLite model
    try:
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        tflite_model = converter.convert()
        
        tflite_path = "models/amulet_model.tflite"
        with open(tflite_path, 'wb') as f:
            f.write(tflite_model)
        
        logger.info(f"üì± TFLite model saved to {tflite_path}")
        logger.info(f"üìè TFLite size: {len(tflite_model) / 1024 / 1024:.2f} MB")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not create TFLite model: {e}")

def test_model_predictions(model, class_names):
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
    logger.info("üß™ Testing model predictions")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_gen = SyntheticDataGenerator(num_samples=20)
    
    for class_id in range(4):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™
        test_img = test_gen.generate_class_pattern(class_id)
        test_img = np.expand_dims(test_img, axis=0)
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        predictions = model.predict(test_img, verbose=0)
        predicted_class = np.argmax(predictions[0])
        confidence = predictions[0][predicted_class]
        
        logger.info(f"üìã Class {class_id} ({class_names[class_id]}):")
        logger.info(f"   - Predicted: {predicted_class} ({class_names[predicted_class]})")
        logger.info(f"   - Confidence: {confidence:.3f}")
        logger.info(f"   - Correct: {'‚úÖ' if predicted_class == class_id else '‚ùå'}")

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    logger.info("ü§ñ Creating Real AI Model with Synthetic Data")
    logger.info("=" * 60)
    
    try:
        # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model, class_names = train_model()
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        save_model(model, class_names)
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
        test_model_predictions(model, class_names)
        
        logger.info("=" * 60)
        logger.info("üéâ SUCCESS! AI Model is ready")
        logger.info("üìÇ Files created:")
        logger.info("   - models/amulet_model.h5 (Keras model)")
        logger.info("   - models/amulet_model.tflite (Mobile model)")
        logger.info("   - labels.json (Class labels)")
        logger.info("")
        logger.info("üöÄ Next steps:")
        logger.info("   1. Start the backend: uvicorn backend.api:app --reload --port 8000")
        logger.info("   2. Start the frontend: streamlit run frontend/app_streamlit.py")
        logger.info("   3. The system will now use REAL AI instead of mock data!")
        
    except Exception as e:
        logger.error(f"‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
