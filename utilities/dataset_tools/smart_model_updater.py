#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Model Updater - ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
"""

import os
import json
import joblib
import numpy as np
from pathlib import Path
from sklearn.preprocessing import LabelEncoder
from datetime import datetime

# Configuration
TRAINED_MODEL_PATH = "trained_model"
LABELS_PATH = "ai_models/labels.json"
DATASET_PATH = "Data set"

def check_existing_models():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß"""
    models = {}
    
    model_files = [
        "classifier.joblib",
        "scaler.joblib", 
        "label_encoder.joblib",
        "pca.joblib",
        "ood_detector.joblib"
    ]
    
    print("üîç Scanning existing models...")
    
    for model_file in model_files:
        model_path = os.path.join(TRAINED_MODEL_PATH, model_file)
        if os.path.exists(model_path):
            try:
                model = joblib.load(model_path)
                models[model_file] = {
                    "path": model_path,
                    "model": model,
                    "size": os.path.getsize(model_path),
                    "modified": datetime.fromtimestamp(os.path.getmtime(model_path))
                }
                print(f"‚úÖ Found: {model_file} ({models[model_file]['size']} bytes)")
            except Exception as e:
                print(f"‚ùå Error loading {model_file}: {e}")
        else:
            print(f"‚ùå Missing: {model_file}")
    
    return models

def check_model_info():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
    info_files = ["model_info.json", "deployment_info.json"]
    info = {}
    
    for info_file in info_files:
        info_path = os.path.join(TRAINED_MODEL_PATH, info_file)
        if os.path.exists(info_path):
            try:
                with open(info_path, 'r', encoding='utf-8') as f:
                    info[info_file] = json.load(f)
                print(f"‚úÖ Found: {info_file}")
            except Exception as e:
                print(f"‚ùå Error reading {info_file}: {e}")
        else:
            print(f"‚ùå Missing: {info_file}")
    
    return info

def check_dataset_structure():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á dataset"""
    dataset_path = Path(DATASET_PATH)
    
    if not dataset_path.exists():
        print(f"‚ùå Dataset path not found: {DATASET_PATH}")
        return {}
    
    print(f"üîç Scanning dataset: {DATASET_PATH}")
    
    classes = {}
    for class_folder in dataset_path.iterdir():
        if class_folder.is_dir():
            # Count image files
            image_files = []
            for ext in ["*.png", "*.jpg", "*.jpeg", "*.PNG", "*.JPG", "*.JPEG"]:
                image_files.extend(list(class_folder.rglob(ext)))
            
            classes[class_folder.name] = {
                "path": str(class_folder),
                "image_count": len(image_files),
                "sample_files": [str(f) for f in image_files[:3]]  # First 3 files as samples
            }
            
            print(f"üìÅ {class_folder.name}: {len(image_files)} images")
    
    return classes

def update_labels_mapping(dataset_classes):
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï labels mapping ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö dataset"""
    if not dataset_classes:
        print("‚ùå No dataset classes found")
        return False
    
    # Load existing labels
    try:
        with open(LABELS_PATH, 'r', encoding='utf-8') as f:
            labels_data = json.load(f)
    except:
        print("‚ùå Could not load labels.json")
        return False
    
    # Update dataset mapping
    labels_data["dataset_mapping"] = {}
    labels_data["current_classes"] = {}
    
    for idx, class_name in enumerate(sorted(dataset_classes.keys())):
        labels_data["dataset_mapping"][class_name] = idx
        labels_data["current_classes"][str(idx)] = class_name
    
    labels_data["total_dataset_classes"] = len(dataset_classes)
    labels_data["last_updated"] = datetime.now().isoformat()
    
    # Save updated labels
    try:
        with open(LABELS_PATH, 'w', encoding='utf-8') as f:
            json.dump(labels_data, f, ensure_ascii=False, indent=2)
        print("‚úÖ Updated labels.json with current dataset")
        return True
    except Exception as e:
        print(f"‚ùå Error updating labels.json: {e}")
        return False

def create_label_encoder_for_dataset(dataset_classes):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á label encoder ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dataset ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô"""
    if not dataset_classes:
        return None
    
    # Create new label encoder
    label_encoder = LabelEncoder()
    class_names = sorted(dataset_classes.keys())
    label_encoder.fit(class_names)
    
    # Save new label encoder
    encoder_path = os.path.join(TRAINED_MODEL_PATH, "dataset_label_encoder.joblib")
    joblib.dump(label_encoder, encoder_path)
    
    print(f"‚úÖ Created new label encoder for dataset: {encoder_path}")
    print(f"üìù Classes: {class_names}")
    
    return label_encoder

def main():
    """Main function - ‡∏™‡πÅ‡∏Å‡∏ô‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°"""
    print("üöÄ Smart Model Updater Starting...")
    print("=" * 50)
    
    # 1. Check existing models
    existing_models = check_existing_models()
    print()
    
    # 2. Check model info
    model_info = check_model_info()
    print()
    
    # 3. Check dataset
    dataset_classes = check_dataset_structure()
    print()
    
    # 4. Update labels if needed
    if dataset_classes:
        print("üìù Updating labels mapping...")
        update_labels_mapping(dataset_classes)
        
        # Create dataset-specific label encoder
        create_label_encoder_for_dataset(dataset_classes)
        print()
    
    # 5. Summary
    print("üìä SUMMARY")
    print("=" * 30)
    print(f"‚úÖ Existing models: {len(existing_models)}")
    print(f"‚úÖ Model info files: {len(model_info)}")
    print(f"‚úÖ Dataset classes: {len(dataset_classes)}")
    
    if len(existing_models) >= 3:  # classifier, scaler, label_encoder
        print("\nüéâ READY TO USE!")
        print("‚úÖ Sufficient models found - no need to retrain")
        print("‚úÖ API can start with existing models")
        
        # Test model loading
        try:
            classifier = joblib.load(os.path.join(TRAINED_MODEL_PATH, "classifier.joblib"))
            scaler = joblib.load(os.path.join(TRAINED_MODEL_PATH, "scaler.joblib"))
            print("‚úÖ Models loaded successfully")
            
            # Show model info
            if "model_info.json" in model_info:
                info = model_info["model_info.json"]
                print(f"üìà Model accuracy: {info.get('model_metrics', {}).get('overall_accuracy', 'N/A')}")
                print(f"üìä Training samples: {info.get('dataset_info', {}).get('samples', 'N/A')}")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Error testing models: {e}")
    
    else:
        print("\n‚ö†Ô∏è INCOMPLETE MODELS")
        print("‚ùå Need to train new models")
    
    print("\nüèÅ Scan Complete!")

if __name__ == "__main__":
    main()