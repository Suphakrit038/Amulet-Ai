"""
üß™ Complete Data Management Examples
====================================

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Data Management System ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

Includes:
- Augmentation pipeline
- Preprocessing
- Quality checking
- Dataset loading
- Sampling & splitting

Author: Amulet-AI Team
Date: October 2025
"""

import sys
from pathlib import Path
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# Add project root
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from data_management.augmentation import create_pipeline_from_preset
from data_management.preprocessing import (
    create_basic_preprocessor,
    create_artifact_preprocessor,
    create_strict_checker
)
from data_management.dataset import (
    AmuletDataset,
    create_balanced_sampler,
    split_dataset_stratified,
    analyze_distribution,
    print_distribution
)


# ============================================================================
# Example 1: Complete Preprocessing Pipeline
# ============================================================================

def example_1_complete_preprocessing():
    """‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á preprocessing pipeline ‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"""
    print("\n" + "="*70)
    print("üìã Example 1: Complete Preprocessing Pipeline")
    print("="*70)
    
    # Create test image
    test_img = Image.new('RGB', (400, 400), color=(100, 120, 140))
    
    # Step 1: Quality check
    print("\nüîç Step 1: Quality Check")
    quality_checker = create_strict_checker()
    metrics = quality_checker.check_quality(test_img)
    
    print(f"  Blur score: {metrics.blur_score:.2f}")
    print(f"  Brightness: {metrics.brightness:.1f}")
    print(f"  Contrast: {metrics.contrast:.1f}")
    print(f"  Overall score: {metrics.overall_score:.1f}/100")
    print(f"  Status: {'‚úÖ PASS' if metrics.passed else '‚ùå FAIL'}")
    
    # Step 2: Advanced preprocessing
    print("\nüî¨ Step 2: Advanced Preprocessing")
    preprocessor = create_artifact_preprocessor()
    enhanced_img = preprocessor(test_img)
    print(f"  Applied: {preprocessor.get_config()}")
    
    # Step 3: Basic preprocessing
    print("\n‚öôÔ∏è Step 3: Basic Preprocessing & Normalization")
    basic_prep = create_basic_preprocessor(image_size=224)
    tensor = basic_prep(enhanced_img)
    print(f"  Output shape: {tensor.shape}")
    print(f"  Value range: [{tensor.min():.3f}, {tensor.max():.3f}]")
    
    print("\n‚úÖ Preprocessing pipeline complete!")


# ============================================================================
# Example 2: Dataset Creation & Splitting
# ============================================================================

def example_2_dataset_splitting():
    """‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á dataset ‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á train/val/test"""
    print("\n" + "="*70)
    print("üìã Example 2: Dataset Creation & Splitting")
    print("="*70)
    
    # Create dummy labels
    print("\nüì¶ Creating dummy dataset...")
    labels = [0]*100 + [1]*80 + [2]*120 + [3]*60 + [4]*40 + [5]*90
    print(f"  Total samples: {len(labels)}")
    print(f"  Number of classes: {len(set(labels))}")
    
    # Analyze original distribution
    print("\nüìä Original Distribution:")
    original_stats = analyze_distribution(labels)
    print_distribution(original_stats)
    
    # Split dataset
    print("\n‚úÇÔ∏è Splitting dataset (70/15/15)...")
    train_idx, val_idx, test_idx = split_dataset_stratified(
        labels,
        train_ratio=0.7,
        val_ratio=0.15,
        test_ratio=0.15,
        random_state=42
    )
    
    # Analyze train split
    print("\nüìä Training Split Distribution:")
    train_stats = analyze_distribution(labels, train_idx)
    print_distribution(train_stats)
    
    # Verify stratification
    print("\n‚úÖ Stratification Verification:")
    print(f"  Original imbalance ratio: {original_stats['imbalance_ratio']:.2f}:1")
    print(f"  Train imbalance ratio: {train_stats['imbalance_ratio']:.2f}:1")
    print(f"  Difference: {abs(original_stats['imbalance_ratio'] - train_stats['imbalance_ratio']):.3f}")


# ============================================================================
# Example 3: Balanced Sampling
# ============================================================================

def example_3_balanced_sampling():
    """‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á balanced sampler"""
    print("\n" + "="*70)
    print("üìã Example 3: Balanced Sampling")
    print("="*70)
    
    # Create imbalanced dataset
    labels = [0]*10 + [1]*100 + [2]*200 + [3]*30 + [4]*5 + [5]*50
    
    print("\nüìä Original Distribution (Highly Imbalanced):")
    from collections import Counter
    counts = Counter(labels)
    for cls, count in sorted(counts.items()):
        print(f"  Class {cls}: {count:3d} samples ({count/len(labels)*100:5.1f}%)")
    
    # Create weighted sampler
    print("\n‚öñÔ∏è Creating weighted sampler...")
    sampler = create_balanced_sampler(labels, strategy='weighted')
    
    # Simulate sampling
    print("\nüé≤ Simulating 1000 samples...")
    sampled_labels = []
    for i, idx in enumerate(sampler):
        sampled_labels.append(labels[idx])
        if i >= 999:
            break
    
    # Analyze sampled distribution
    print("\nüìä Sampled Distribution (After Balancing):")
    sampled_counts = Counter(sampled_labels)
    for cls, count in sorted(sampled_counts.items()):
        print(f"  Class {cls}: {count:3d} samples ({count/len(sampled_labels)*100:5.1f}%)")
    
    print("\n‚úÖ Classes are now more balanced!")


# ============================================================================
# Example 4: Full Training Pipeline
# ============================================================================

def example_4_full_pipeline():
    """‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á pipeline ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training"""
    print("\n" + "="*70)
    print("üìã Example 4: Full Training Pipeline")
    print("="*70)
    
    print("\nüîß Pipeline Components:")
    print("  1Ô∏è‚É£  Augmentation: medium preset")
    print("  2Ô∏è‚É£  Preprocessing: artifact preprocessor")
    print("  3Ô∏è‚É£  Quality check: strict checker")
    print("  4Ô∏è‚É£  Sampling: weighted balanced sampler")
    
    # Create augmentation pipeline
    print("\nüé® Creating augmentation pipeline...")
    aug_pipeline = create_pipeline_from_preset(
        'medium',
        batch_size=32,
        num_classes=6,
        image_size=224
    )
    
    stats = aug_pipeline.get_augmentation_stats()
    print(f"  ‚úÖ RandAugment: n={stats['rand_augment_ops']}, m={stats['rand_augment_magnitude']}")
    print(f"  ‚úÖ RandomErasing: p={stats['random_erasing_prob']:.2f}")
    print(f"  ‚úÖ MixUp: Œ±={stats['mixup_alpha']:.2f}")
    print(f"  ‚úÖ CutMix: Œ±={stats['cutmix_alpha']:.2f}")
    
    # Create preprocessing
    print("\nüî¨ Creating preprocessing pipeline...")
    preprocessor = create_artifact_preprocessor()
    config = preprocessor.get_config()
    print(f"  ‚úÖ Denoising: {config['enable_denoise']}")
    print(f"  ‚úÖ CLAHE: {config['enable_clahe']}")
    print(f"  ‚úÖ Edge enhance: {config['enable_edge_enhance']}")
    
    # Create quality checker
    print("\nüîç Creating quality checker...")
    quality_checker = create_strict_checker()
    print(f"  ‚úÖ Strict quality thresholds enabled")
    
    # Create dummy labels
    labels = [0]*100 + [1]*80 + [2]*120 + [3]*60 + [4]*40 + [5]*90
    
    # Create sampler
    print("\n‚öñÔ∏è Creating balanced sampler...")
    sampler = create_balanced_sampler(labels, strategy='weighted')
    print(f"  ‚úÖ Weighted sampling for {len(set(labels))} classes")
    
    print("\nüöÄ Pipeline ready for training!")
    print("\nüìù Usage:")
    print("  1. Load images")
    print("  2. Run quality check (filter bad images)")
    print("  3. Apply preprocessing (CLAHE, denoising, edge enhance)")
    print("  4. Apply augmentation (RandAugment, MixUp/CutMix)")
    print("  5. Use balanced sampler in DataLoader")
    print("  6. Train model!")


# ============================================================================
# Example 5: Quality Check Batch
# ============================================================================

def example_5_quality_check_batch():
    """‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö quality ‡πÅ‡∏ö‡∏ö batch"""
    print("\n" + "="*70)
    print("üìã Example 5: Batch Quality Checking")
    print("="*70)
    
    # Create checker
    quality_checker = create_strict_checker()
    
    # Create test images with different qualities
    print("\nüñºÔ∏è Creating test images...")
    test_images = []
    
    # Good image
    good_img = Image.new('RGB', (400, 400), color=(120, 140, 160))
    test_images.append(("Good", good_img))
    
    # Low resolution
    low_res = Image.new('RGB', (100, 100), color=(120, 140, 160))
    test_images.append(("Low Res", low_res))
    
    # Low contrast
    low_contrast = Image.new('RGB', (400, 400), color=(128, 128, 128))
    test_images.append(("Low Contrast", low_contrast))
    
    # Check each image
    print("\nüîç Quality Check Results:")
    print("-" * 70)
    
    for name, img in test_images:
        metrics = quality_checker.check_quality(img)
        status = "‚úÖ PASS" if metrics.passed else "‚ùå FAIL"
        
        print(f"\n{name}:")
        print(f"  Status: {status}")
        print(f"  Overall score: {metrics.overall_score:.1f}/100")
        print(f"  Resolution: {metrics.resolution[0]}x{metrics.resolution[1]}")
        print(f"  Blur score: {metrics.blur_score:.2f}")
        print(f"  Brightness: {metrics.brightness:.1f}")
        print(f"  Contrast: {metrics.contrast:.1f}")
        
        if metrics.issues:
            print(f"  Issues:")
            for issue in metrics.issues:
                print(f"    ‚Ä¢ {issue}")
    
    print("\n" + "-" * 70)
    print("‚úÖ Batch quality check complete!")


# ============================================================================
# Example 6: Performance Comparison
# ============================================================================

def example_6_performance_comparison():
    """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡∏Ç‡∏≠‡∏á presets ‡∏ï‡πà‡∏≤‡∏á‡πÜ"""
    print("\n" + "="*70)
    print("üìã Example 6: Augmentation Preset Comparison")
    print("="*70)
    
    presets = ['minimal', 'light', 'medium', 'heavy']
    
    print("\nüìä Preset Comparison:")
    print("-" * 70)
    
    for preset in presets:
        pipeline = create_pipeline_from_preset(preset, batch_size=32)
        stats = pipeline.get_augmentation_stats()
        
        print(f"\nüéØ {preset.upper()} Preset:")
        print(f"  RandAugment: {'‚úÖ' if stats['rand_augment_enabled'] else '‚ùå'} "
              f"(n={stats['rand_augment_ops']}, m={stats['rand_augment_magnitude']})")
        print(f"  RandomErasing: {'‚úÖ' if stats['random_erasing_enabled'] else '‚ùå'} "
              f"(p={stats['random_erasing_prob']:.2f})")
        print(f"  MixUp: {'‚úÖ' if stats['mixup_enabled'] else '‚ùå'} "
              f"(Œ±={stats['mixup_alpha']:.2f})")
        print(f"  CutMix: {'‚úÖ' if stats['cutmix_enabled'] else '‚ùå'} "
              f"(Œ±={stats['cutmix_alpha']:.2f})")
    
    print("\n" + "-" * 70)
    print("\nüí° Recommendations:")
    print("  ‚Ä¢ minimal: Fast inference, no augmentation")
    print("  ‚Ä¢ light: Small datasets (100-500 images)")
    print("  ‚Ä¢ medium: Medium datasets (500-2000 images) ‚≠ê RECOMMENDED")
    print("  ‚Ä¢ heavy: Large datasets (>2000 images) or high augmentation needed")


# ============================================================================
# Main: Run All Examples
# ============================================================================

def main():
    """‡∏£‡∏±‡∏ô examples ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    print("\n" + "="*70)
    print("üöÄ Amulet-AI Complete Data Management Examples")
    print("="*70)
    
    try:
        # Run all examples
        example_1_complete_preprocessing()
        example_2_dataset_splitting()
        example_3_balanced_sampling()
        example_4_full_pipeline()
        example_5_quality_check_batch()
        example_6_performance_comparison()
        
        print("\n" + "="*70)
        print("‚úÖ All examples completed successfully!")
        print("="*70)
        
        print("\nüìö Next Steps:")
        print("  1. Try with real Amulet images from organized_dataset/")
        print("  2. Tune augmentation presets for your dataset size")
        print("  3. Implement Phase 2: Transfer Learning")
        print("  4. Start training your model!")
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
