"""
Web scraping system using Scrapy for collecting amulet market data
Scrapes prices, market trends, and other relevant data from various sources
"""
import scrapy
import json
import csv
import os
import re
from datetime import datetime
from typing import Dict, List, Optional
import requests
from bs4 import BeautifulSoup
import logging

logger = logging.getLogger(__name__)

class AmuletPriceSpider(scrapy.Spider):
    """Spider for scraping amulet prices from various sources"""
    
    name = 'amulet_prices'
    allowed_domains = ['example-amulet-site.com']  # Replace with actual domains
    
    def __init__(self, search_terms=None, *args, **kwargs):
        super(AmuletPriceSpider, self).__init__(*args, **kwargs)
        self.search_terms = search_terms or [
            'à¸«à¸¥à¸§à¸‡à¸žà¹ˆà¸­à¸à¸§à¸¢à¹à¸«à¸§à¸à¸¡à¹ˆà¸²à¸™',
            'à¹‚à¸žà¸˜à¸´à¹Œà¸à¸²à¸™à¸šà¸±à¸§',
            'à¸à¸²à¸™à¸ªà¸´à¸‡à¸«à¹Œ',
            'à¸žà¸£à¸°à¸ªà¸µà¸§à¸¥à¸µ'
        ]
        self.scraped_data = []
    
    def start_requests(self):
        """Generate initial requests for each search term"""
        base_urls = [
            'https://www.example-market.com/search?q={}',
            'https://www.another-market.com/search?keyword={}',
            # Add more marketplace URLs here
        ]
        
        for term in self.search_terms:
            for url_template in base_urls:
                url = url_template.format(term)
                yield scrapy.Request(
                    url=url,
                    callback=self.parse_search_results,
                    meta={'search_term': term}
                )
    
    def parse_search_results(self, response):
        """Parse search results page"""
        search_term = response.meta['search_term']
        
        # Example selectors - adjust based on actual site structure
        item_links = response.css('.item-link::attr(href)').getall()
        
        for link in item_links:
            full_url = response.urljoin(link)
            yield scrapy.Request(
                url=full_url,
                callback=self.parse_item_details,
                meta={'search_term': search_term}
            )
        
        # Follow pagination
        next_page = response.css('.next-page::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse_search_results)
    
    def parse_item_details(self, response):
        """Parse individual item details"""
        search_term = response.meta['search_term']
        
        # Extract item details - adjust selectors based on actual site
        item_data = {
            'search_term': search_term,
            'title': self.clean_text(response.css('.item-title::text').get()),
            'price': self.extract_price(response.css('.price::text').get()),
            'description': self.clean_text(response.css('.description::text').get()),
            'seller': self.clean_text(response.css('.seller-name::text').get()),
            'location': self.clean_text(response.css('.location::text').get()),
            'date_posted': self.extract_date(response.css('.date-posted::text').get()),
            'url': response.url,
            'scraped_at': datetime.now().isoformat(),
            'images': response.css('.item-images img::attr(src)').getall(),
            'condition': self.extract_condition(response.css('.condition::text').get()),
            'temple': self.extract_temple_info(response.css('.item-details::text').getall())
        }
        
        # Validate required fields
        if item_data['price'] and item_data['title']:
            self.scraped_data.append(item_data)
            yield item_data
    
    def clean_text(self, text):
        """Clean extracted text"""
        if not text:
            return None
        return re.sub(r'\s+', ' ', text.strip())
    
    def extract_price(self, price_text):
        """Extract numeric price from text"""
        if not price_text:
            return None
        
        # Remove currency symbols and extract numbers
        price_match = re.search(r'[\d,]+', price_text.replace(',', ''))
        if price_match:
            try:
                return float(price_match.group().replace(',', ''))
            except ValueError:
                return None
        return None
    
    def extract_date(self, date_text):
        """Extract and standardize date"""
        if not date_text:
            return None
        
        # Add date parsing logic based on site format
        # This is a simple example
        return date_text.strip()
    
    def extract_condition(self, condition_text):
        """Extract item condition"""
        if not condition_text:
            return None
        
        condition_text = condition_text.lower()
        if any(word in condition_text for word in ['à¹ƒà¸«à¸¡à¹ˆ', 'new']):
            return 'à¹ƒà¸«à¸¡à¹ˆ'
        elif any(word in condition_text for word in ['à¹€à¸à¹ˆà¸²', 'old', 'vintage']):
            return 'à¹€à¸à¹ˆà¸²'
        else:
            return 'à¹ƒà¸Šà¹‰à¹à¸¥à¹‰à¸§'
    
    def extract_temple_info(self, details_list):
        """Extract temple information from item details"""
        if not details_list:
            return None
        
        details_text = ' '.join(details_list).lower()
        
        # Common temple patterns
        temple_patterns = [
            r'à¸§à¸±à¸”[^\s]+',
            r'temple [^\s]+',
            r'à¸žà¸£à¸°[^\s]+à¸§à¸±à¸”[^\s]+',
        ]
        
        for pattern in temple_patterns:
            match = re.search(pattern, details_text)
            if match:
                return match.group()
        
        return None

class MarketDataCollector:
    """High-level interface for collecting market data"""
    
    def __init__(self, output_dir: str = "data/scraped"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def collect_market_data(self, search_terms: List[str] = None) -> str:
        """
        Collect market data using Scrapy
        
        Args:
            search_terms: List of search terms to scrape
            
        Returns:
            Path to output file
        """
        if search_terms is None:
            search_terms = ['à¸«à¸¥à¸§à¸‡à¸žà¹ˆà¸­à¸à¸§à¸¢à¹à¸«à¸§à¸à¸¡à¹ˆà¸²à¸™', 'à¹‚à¸žà¸˜à¸´à¹Œà¸à¸²à¸™à¸šà¸±à¸§', 'à¸à¸²à¸™à¸ªà¸´à¸‡à¸«à¹Œ', 'à¸žà¸£à¸°à¸ªà¸µà¸§à¸¥à¸µ']
        
        output_file = os.path.join(
            self.output_dir, 
            f"market_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        
        # Note: In a real implementation, you would run Scrapy programmatically
        # For now, we'll create mock data
        mock_data = self._generate_mock_market_data(search_terms)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(mock_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ðŸ’¾ Market data saved to {output_file}")
        return output_file
    
    def _generate_mock_market_data(self, search_terms: List[str]) -> List[Dict]:
        """Generate mock market data for development"""
        import random
        
        mock_data = []
        
        for term in search_terms:
            for i in range(random.randint(10, 30)):
                base_price = {
                    'à¸«à¸¥à¸§à¸‡à¸žà¹ˆà¸­à¸à¸§à¸¢à¹à¸«à¸§à¸à¸¡à¹ˆà¸²à¸™': 8000,
                    'à¹‚à¸žà¸˜à¸´à¹Œà¸à¸²à¸™à¸šà¸±à¸§': 6000,
                    'à¸à¸²à¸™à¸ªà¸´à¸‡à¸«à¹Œ': 4000,
                    'à¸žà¸£à¸°à¸ªà¸µà¸§à¸¥à¸µ': 3000
                }.get(term, 5000)
                
                mock_item = {
                    'search_term': term,
                    'title': f"{term} à¸£à¸¸à¹ˆà¸™à¸žà¸´à¹€à¸¨à¸© à¸›à¸µ {random.randint(2520, 2565)}",
                    'price': base_price * random.uniform(0.5, 3.0),
                    'description': f"à¸žà¸£à¸°à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡ {term} à¸ªà¸ à¸²à¸žà¸”à¸µ à¸¡à¸µà¸›à¸£à¸°à¸§à¸±à¸•à¸´",
                    'seller': f"à¸£à¹‰à¸²à¸™{random.choice(['à¸žà¸£à¸°à¸”à¸µ', 'à¹€à¸ˆà¹‰à¸²à¹à¸¡à¹ˆ', 'à¸šà¸¸à¸à¸™à¸´à¸¡à¸´à¸•', 'à¸¨à¸£à¸±à¸”à¸˜à¸²'])}",
                    'location': random.choice(['à¸à¸£à¸¸à¸‡à¹€à¸—à¸žà¸¯', 'à¸™à¸™à¸—à¸šà¸¸à¸£à¸µ', 'à¸ªà¸¡à¸¸à¸—à¸£à¸›à¸£à¸²à¸à¸²à¸£', 'à¸Šà¸¥à¸šà¸¸à¸£à¸µ']),
                    'date_posted': f"2024-{random.randint(1,12):02d}-{random.randint(1,28):02d}",
                    'condition': random.choice(['à¹ƒà¸«à¸¡à¹ˆ', 'à¹ƒà¸Šà¹‰à¹à¸¥à¹‰à¸§', 'à¹€à¸à¹ˆà¸²']),
                    'temple': f"à¸§à¸±à¸”{random.choice(['à¹à¸ªà¸‡à¸­à¸£à¸¸à¸“', 'à¹ƒà¸«à¸à¹ˆ', 'à¹€à¸à¹ˆà¸²', 'à¹ƒà¸«à¸¡à¹ˆ', 'à¸šà¸¹à¸£à¸žà¸²'])}",
                    'scraped_at': datetime.now().isoformat()
                }
                
                mock_data.append(mock_item)
        
        return mock_data
    
    def load_market_data(self, file_path: str) -> List[Dict]:
        """Load market data from file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ Could not load market data: {e}")
            return []
    
    def get_latest_market_data(self) -> List[Dict]:
        """Get the most recent market data file"""
        try:
            files = [f for f in os.listdir(self.output_dir) if f.startswith('market_data_')]
            if not files:
                logger.info("ðŸ”„ No market data found, collecting new data")
                file_path = self.collect_market_data()
                return self.load_market_data(file_path)
            
            latest_file = max(files)
            file_path = os.path.join(self.output_dir, latest_file)
            return self.load_market_data(file_path)
            
        except Exception as e:
            logger.error(f"âŒ Could not get market data: {e}")
            return []
    
    def analyze_market_trends(self, class_name: str) -> Dict:
        """
        Analyze market trends for specific amulet class
        
        Args:
            class_name: Name of amulet class
            
        Returns:
            Dictionary with market analysis
        """
        data = self.get_latest_market_data()
        
        # Filter data for specific class
        class_data = [item for item in data if item.get('search_term') == class_name]
        
        if not class_data:
            return {
                'average_price': 5000,
                'min_price': 1000,
                'max_price': 15000,
                'market_activity': 'low',
                'trend': 'stable',
                'sample_size': 0
            }
        
        prices = [item['price'] for item in class_data if isinstance(item.get('price'), (int, float))]
        
        if not prices:
            return {
                'average_price': 5000,
                'min_price': 1000,
                'max_price': 15000,
                'market_activity': 'low',
                'trend': 'stable',
                'sample_size': 0
            }
        
        return {
            'average_price': sum(prices) / len(prices),
            'min_price': min(prices),
            'max_price': max(prices),
            'market_activity': 'high' if len(prices) > 20 else 'medium' if len(prices) > 10 else 'low',
            'trend': 'stable',  # Could implement trend analysis
            'sample_size': len(prices),
            'conditions': list(set(item.get('condition', 'à¹ƒà¸Šà¹‰à¹à¸¥à¹‰à¸§') for item in class_data)),
            'top_locations': list(set(item.get('location', 'à¸à¸£à¸¸à¸‡à¹€à¸—à¸žà¸¯') for item in class_data))[:5]
        }

# Integration functions
def get_market_insights(class_name: str) -> Dict:
    """
    Get market insights for API integration
    
    Args:
        class_name: Name of amulet class
        
    Returns:
        Market insights dictionary
    """
    collector = MarketDataCollector()
    return collector.analyze_market_trends(class_name)

def update_market_data():
    """Update market data collection"""
    collector = MarketDataCollector()
    collector.collect_market_data()
    logger.info("âœ… Market data updated")

# Scrapy settings (if running programmatically)
SCRAPY_SETTINGS = {
    'ROBOTSTXT_OBEY': True,
    'DOWNLOAD_DELAY': 1,
    'RANDOMIZE_DOWNLOAD_DELAY': True,
    'CONCURRENT_REQUESTS': 1,
    'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
    'USER_AGENT': 'AmuletAI Bot 1.0',
    'DEFAULT_REQUEST_HEADERS': {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'th,en',
    }
}
